{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**PYTHON ADVANCED ASSIGNMENT 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTOR : VAHID HADAVI\n",
    "\n",
    "### STUDENTS:\n",
    "* ALISHA THOMAS\n",
    "* ASWATHY ANILKUMAR\n",
    "* JASMIN JOSEPH\n",
    "* SUDHY SUKUMARAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.SCRAPING DATA FROM KIJIJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Installing required libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install bs4\n",
    "# !pip install geopy\n",
    "# !pip install geopy matplotlib\n",
    "# !pip install pandas-profiling\n",
    "# !pip install ydata-profiling\n",
    "# !pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'missingno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Visualization libraries\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# from pandas_profiling import ProfileReport\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmissingno\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmsno\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'missingno'"
     ]
    }
   ],
   "source": [
    "# For query a website\n",
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# To parse a html page and extract required data\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "# from geopy.geocoders import Nominatim\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Visualization libraries\n",
    "# from pandas_profiling import ProfileReport\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "\n",
    "#Nlp libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "# Load the language model\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Scraping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#function to get values having datatype text,content,or datetime else,it will return 'null'\n",
    "\n",
    "def findOrNan(obj,type):\n",
    "    if type=='text':\n",
    "        return obj.text if hasattr(obj,'text') else 'null'\n",
    "    elif type == 'content':\n",
    "        return obj['content'] if hasattr(obj,'content') else 'null'\n",
    "    elif type== 'datetime':\n",
    "        return obj['datetime'] if hasattr(obj,'datetime') else 'null'\n",
    "\n",
    "\n",
    "#  To get the data from first 200 pages of kijiji long term rental inside ontario\n",
    "\n",
    "entries=[]\n",
    "iter=0\n",
    "\n",
    "for i in range(1,200):\n",
    "   url = f\"https://www.kijiji.ca/b-apartments-condos/ontario/page-{i}/c37l9004\"\n",
    "\n",
    "   webPage = urllib.request.urlopen(url)\n",
    "   parsedPage = BeautifulSoup(webPage)\n",
    "\n",
    "   item_list = parsedPage.find_all('ul', attrs={'data-testid': 'srp-search-list'})\n",
    "   for list in item_list:\n",
    "      # Finding each <a></a> tag and extracting id=\"listing-link\" for getting more details of the product\n",
    "      a_tags_with_link_id = list.find_all('a', attrs={\"data-testid\":\"listing-link\"})\n",
    "      for tag in a_tags_with_link_id:\n",
    "\n",
    "         iter=iter+1\n",
    "         # print(\"https://www.kijiji.ca\"+tag['href'])\n",
    "         page_details = {}\n",
    "         # ad_id=\"\"\n",
    "\n",
    "\n",
    "         listing = urllib.request.urlopen(\"https://www.kijiji.ca\"+tag['href'])\n",
    "         # listing = urllib.request.urlopen(\"https://www.kijiji.ca/v-apartments-condos/city-of-toronto/annex-1-bedroom-with-private-backyard/1685925449\")\n",
    "\n",
    "         \n",
    "         page_details_html= BeautifulSoup(listing, 'html.parser')\n",
    "\n",
    "         # id = page_details_html.find('li', class_='currentCrumb-2810804557')\n",
    "         # if id:\n",
    "         #    ad_id = findOrNan(id.find('a', class_='adId-1017171818'),'text')\n",
    "         #    print(\"ad_id\",ad_id)\n",
    "\n",
    "         # Extracting usefull details from HTML page\n",
    "         title = findOrNan(page_details_html.find('h1', class_='title-4206718449'),'text')\n",
    "         page_details['title'] = title\n",
    "\n",
    "         price = findOrNan(page_details_html.find('span', content=True),'content')\n",
    "         page_details['price'] = price\n",
    "\n",
    "         price_condition =findOrNan(page_details_html.find('span', class_='utilities-3805585157'),'text')\n",
    "         page_details['price_condition'] = price_condition\n",
    "\n",
    "         address = findOrNan(page_details_html.find('span', itemprop='address'),'text') \n",
    "         page_details['address'] = address\n",
    "\n",
    "         posted_date = findOrNan(page_details_html.find('time'),'datetime')\n",
    "         page_details['posted_date'] = posted_date\n",
    "\n",
    "\n",
    "         quick_view = page_details_html.find('div', class_='titleAttributes-183069789')\n",
    "         if quick_view:\n",
    "            for quick_view_detail in quick_view.find_all('li'):\n",
    "               quick_view_detail_value = findOrNan(quick_view_detail.find('span', class_='noLabelValue-774086477'),'text')\n",
    "               if('Bedrooms' in quick_view_detail_value):\n",
    "                  page_details['Bedrooms'] = quick_view_detail_value\n",
    "               elif 'Bathrooms' in quick_view_detail_value:\n",
    "                  page_details['Bathrooms'] = quick_view_detail_value\n",
    "               else:\n",
    "                  page_details['unit_type'] = quick_view_detail_value\n",
    "\n",
    "\n",
    "         utilities_included = page_details_html.find('h4', string='Utilities Included')\n",
    "         pattern = r'(Yes|No): (.*)'\n",
    "         if utilities_included:\n",
    "            utilities_list = utilities_included.find_next('ul').find_all('li')\n",
    "            for utility in utilities_list:\n",
    "               svg_tag = utility.find('svg')\n",
    "               if svg_tag:\n",
    "                  aria_label = svg_tag.get('aria-label')\n",
    "                  match = re.match(pattern, aria_label)\n",
    "                  if match:\n",
    "                     value = match.group(1)\n",
    "                     label = match.group(2)\n",
    "                     page_details[label] = value\n",
    "\n",
    "\n",
    "         wifi_and_more = page_details_html.find('h4', string='Wi-Fi and More')\n",
    "         if wifi_and_more:\n",
    "            parent_div = wifi_and_more.find_parent('div')\n",
    "\n",
    "            features_list = parent_div.find_next('ul').find_all('li')\n",
    "            for feature in features_list:\n",
    "               page_details[feature.text] = True\n",
    "\n",
    "         parking = page_details_html.find('dt', string='Parking Included')\n",
    "         if parking:\n",
    "            features_list = parking.find_next('dd')\n",
    "            page_details['parking'] = findOrNan(features_list,'text')\n",
    "\n",
    "         agreement = page_details_html.find('dt', string='Agreement Type')\n",
    "         if agreement:\n",
    "            agreement_type = agreement.find_next('dd')\n",
    "            page_details['agreement_type'] = findOrNan(agreement_type,'text')\n",
    "\n",
    "         moveIn = page_details_html.find('dt', string='Move-In Date')\n",
    "         if moveIn:\n",
    "            moveIn_date = moveIn.find_next('dd')\n",
    "            page_details['moveIn_date'] = findOrNan(moveIn_date,'text')\n",
    "\n",
    "         pet = page_details_html.find('dt', string='Pet Friendly')\n",
    "         if pet:\n",
    "            pet_friendly = pet.find_next('dd')\n",
    "            page_details['pet_friendly'] = findOrNan(pet_friendly,'text')\n",
    "\n",
    "         size = page_details_html.find('dt', string='Size (sqft)')\n",
    "         if size:\n",
    "            size_sqrt = size.find_next('dd')\n",
    "            page_details['size_sqrt'] = findOrNan(size_sqrt,'text')\n",
    "\n",
    "         furnished = page_details_html.find('dt', string='Furnished')\n",
    "         if furnished:\n",
    "            furnished_info = furnished.find_next('dd')\n",
    "            page_details['furnished_info'] = findOrNan(furnished_info,'text')\n",
    "\n",
    "         appliances = page_details_html.find('h4', string='Appliances')\n",
    "         if appliances:\n",
    "            parent_div = appliances.find_parent('div')\n",
    "\n",
    "            appliances_list = parent_div.find_next('ul').find_all('li')\n",
    "            for feat in appliances_list:\n",
    "               page_details[feat.text] = True\n",
    "\n",
    "         air_conditioning = page_details_html.find('dt', string='Air Conditioning')\n",
    "         if air_conditioning:\n",
    "            air_conditioning_info = air_conditioning.find_next('dd')\n",
    "            page_details['air_conditioning_info'] = findOrNan(air_conditioning_info,'text')\n",
    "\n",
    "         personal_outdoor_space = page_details_html.find('dt', string='Personal Outdoor Space')\n",
    "         if personal_outdoor_space:\n",
    "            personal_outdoor_space_info = personal_outdoor_space.find_next('dd')\n",
    "            page_details['personal_outdoor_space_info'] = findOrNan(personal_outdoor_space_info,'text')\n",
    "\n",
    "         smoking = page_details_html.find('dt', string='Smoking Permitted')\n",
    "         if smoking:\n",
    "            smoking_info = smoking.find_next('dd')\n",
    "            page_details['smoking_info'] = findOrNan(smoking_info,'text')\n",
    "\n",
    "         amenities = page_details_html.find('h4', string='Amenities')\n",
    "         if amenities:\n",
    "            parent_div = amenities.find_parent('div')\n",
    "\n",
    "            amenities_list = parent_div.find_next('ul').find_all('li')\n",
    "            for feat in amenities_list:\n",
    "               page_details[feat.text] = True\n",
    "\n",
    "\n",
    "\n",
    "         description_section = page_details_html.find('h3', string='Description')\n",
    "         if description_section:\n",
    "            parent_div = description_section.find_parent('div')\n",
    "            description_text = parent_div.get_text(separator='\\n')\n",
    "            page_details['description']=description_text\n",
    "\n",
    "\n",
    "         # entries.append(page_details)\n",
    "         # print(\"-------------------------\",page_details)\n",
    "         # print(\">>>>\",entries)\n",
    "         with open('vals.txt', 'a',encoding='utf-8') as file:\n",
    "            file.write(str(page_details) + '\\n')\n",
    "\n",
    "         if iter % 10 == 0:  # Save every 100 iterations\n",
    "            with open('loop_state.pkl', 'wb') as file:\n",
    "                  pickle.dump(i, file)\n",
    "\n",
    "         delay = random.randint(10, 30)\n",
    "         time.sleep(delay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Converting scraped text file to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store data\n",
    "price = []\n",
    "title = []\n",
    "price_condition=[]\n",
    "posted_date=[]\n",
    "Bedrooms=[]\n",
    "Bathrooms=[]\n",
    "unit_type=[]\n",
    "Hydro=[]\n",
    "Heat=[]\n",
    "Water=[]\n",
    "Cable_TV=[]\n",
    "Internet=[]\n",
    "parking=[]\n",
    "agreement_type=[]\n",
    "moveIn_date=[]\n",
    "pet_friendly=[]\n",
    "size_sqft=[]\n",
    "furnished_info=[]\n",
    "description=[]\n",
    "# Read the content of the text file and extract data\n",
    "with open('_com3.txt', 'rb') as file:\n",
    "#     with open('file.txt', 'rb') as f:\n",
    "#     byte_sequence = f.read()\n",
    "\n",
    "# decoded_text = byte_sequence.decode('utf-8')\n",
    "\n",
    "    for line in file:\n",
    "        data = eval(line)  # Convert string to dictionary\n",
    "        price.append(data['price'])\n",
    "        title.append(data['title'])\n",
    "        price_condition.append(data['price_condition'])\n",
    "       \n",
    "        posted_date.append(data['posted_date'])\n",
    "\n",
    "        Bedrooms.append(data.get('Bedrooms'))\n",
    "        Bathrooms.append(data.get('Bathrooms'))\n",
    "        \n",
    "        unit_type.append(data.get('unit_type','N/A'))\n",
    "        \n",
    "        Hydro.append(data.get('Hydro'))\n",
    "        Heat.append(data.get('Heat'))\n",
    "        Water.append(data.get('Water'))\n",
    "        Cable_TV.append(data.get('Cable / TV'))\n",
    "        Internet.append(data.get('Internet'))\n",
    "        parking.append(data.get('parking'))\n",
    "        agreement_type.append(data.get('agreement_type'))\n",
    "        moveIn_date.append(data.get('moveIn_date'))\n",
    "        pet_friendly.append(data.get('pet_friendly'))\n",
    "        size_sqft.append(data.get('size_sqrt') or data.get('size_sqft'))\n",
    "       \n",
    "        furnished_info.append(data.get('furnished_info'))\n",
    "        description.append(data.get('description'))\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame({'Title':title,'Price_condition':price_condition,\n",
    "                   'Unit_type':unit_type,'Bedrooms':Bedrooms,'Bathrooms':Bathrooms,'Hydro':Hydro,\n",
    "                   'Heat':Heat,'Water':Water,'Cable / TV':Cable_TV,'Internet':Internet,'Parking':parking,\n",
    "                   'Agreement_type':agreement_type,\n",
    "                   'MoveIn_date':moveIn_date,'Pet_friendly':pet_friendly,'Size_sqft':size_sqft,\n",
    "                   'Furnished_info':furnished_info,'Price':price,'Description':description })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert it to csv file for storing the data\n",
    "df.to_csv(\"property_rental_dataframe.csv\",index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data wranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the dataset\n",
    "df = pd.read_csv(\"property_rental_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "#shape of dataframe after removing duplicates\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the concise summary about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get the names of each columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values in each columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the null values\n",
    "msno.matrix(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Black color in the above graph indicates non-null values and white indicate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check all unique values and their counts\n",
    "\n",
    "unique_val_check_colomns = ['Price_condition','Size_sqft', 'Unit_type', 'Bedrooms', 'Bathrooms', \n",
    "                    'Hydro', 'Heat', 'Water', 'Parking','Cable / TV','Internet',\n",
    "                    'Furnished_info', 'Pet_friendly','Agreement_type']\n",
    "\n",
    "for column in unique_val_check_colomns:\n",
    "    print(column + ': \\n', df[column].value_counts(),'\\n------------------------------------------------------------------------\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After analysing unique values, In feature Bedrooms, found noise (Values from feature \"Bathrooms\"). Hence we removed the noise data.\n",
    "noise= ['Bathrooms: 2','Bathrooms: 1.5','Bathrooms: 2.5','Bathrooms: 3','Bathrooms: 3.5']\n",
    "index = df.index[df['Bedrooms'].isin(noise)]\n",
    "df.drop(index=index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Water, Hydro,Heat, Internet and CableTv are the utilities. More than half of the data is missing. Instead of using indivival utilities, the other column 'Utility_included' is considering for further analysis.\n",
    "##Hence individual utilities are dropped from the dataset\n",
    "\n",
    "df.drop(columns=['Hydro','Water','Heat','Cable / TV','Internet'], inplace=True)\n",
    "\n",
    "# Handling missing values\n",
    "# Majority of the data is missing  in 'MoveIn_date' & 'Agreement_type' and it is not a big factor associated with house rentals\n",
    "df.drop(columns=['MoveIn_date','Agreement_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handle missing values\n",
    "## We drop the rows having missing values except for 'Size_sqft', because 'Size_sqft' have more number of missing values.So droping those will result in more data lose.\n",
    "\n",
    "df.dropna( subset = [ 'Title' , 'Price_condition' , 'Unit_type' , 'Bedrooms','Bathrooms','Parking','Pet_friendly','Furnished_info','Price','Description'] , inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of Unit_type have value 'Not Available'\n",
    "len(df[df['Unit_type']=='Not Available'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here in feature 'Unit_type', Not available' can be cosider as null value.So those rows were droped. \n",
    "df.drop(df[df['Unit_type'] == 'Not Available'].index, inplace=True)\n",
    "\n",
    "# Replacing values in bedroom to numerical currespoding values\n",
    "df['Bedrooms'].replace(['Bedrooms: Bachelor/Studio',\n",
    "                        'Bedrooms: 1','Bedrooms: 1 + Den',\n",
    "                        'Bedrooms: 2','Bedrooms: 2 + Den',\n",
    "                        'Bedrooms: 3' , 'Bedrooms: 3 + Den',\n",
    "                        'Bedrooms: 4','Bedrooms: 4 + Den',\n",
    "                        'Bedrooms: 5+'],[0.5,1,1.5,2,2.5,3,3.5,4,4.5,5], inplace=True)\n",
    "\n",
    "# Replacing values in bathroom to numerical curresponding values\n",
    "df['Bathrooms'].replace(['Bathrooms: 1' ,'Bathrooms: 1.5' ,'Bathrooms: 2' ,\n",
    "                         'Bathrooms: 2.5','Bathrooms: 3', 'Bathrooms: 3.5' ,\n",
    "                         'Bathrooms: 4' ,'Bathrooms: 4.5','Bathrooms: 5','Bathrooms: 6+'],\n",
    "                          [1,1.5,2,2.5,3,3.5,4,4.5,5,6],inplace=True)\n",
    "\n",
    "#replacing values in parrking to numerical curresponding values\n",
    "df['Parking'].replace(['3+'],['3'],inplace=True)\n",
    "\n",
    "#convert type of parking to integer type\n",
    "df['Parking']=df['Parking'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking the percent of the column 'Size_sqft' having values null or 'Not Available'\n",
    "print('Count of Not Available: ',(df[\"Size_sqft\"]=='Not Available').sum())\n",
    "print('Count of null values: ',df['Size_sqft'].isnull().sum())\n",
    "x= ((df[\"Size_sqft\"]=='Not Available').sum())+(df['Size_sqft'].isnull().sum())\n",
    "null_percent=(x/df.shape[0])*100\n",
    "print(\"Null percent: \",null_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the missing values after removal of null values except \"Size_sqft\"\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this dataset 'Not Available' in feaure 'Size_sqft' can be cosider as null value. Since the number of missing values are high and the feature is important, the missing values is going to impute using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Not Available' as Null value\n",
    "df['Size_sqft'] = df['Size_sqft'].replace('Not Available', np.nan)\n",
    "\n",
    "## Convert size_sqrt into int type\n",
    "def size_int_converter(value):\n",
    "    if isinstance(value, str):\n",
    "        return int(value.replace(\",\", \"\"))\n",
    "    elif isinstance(value, int):\n",
    "        return value\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "df['Size_sqft'] = df['Size_sqft'].apply(size_int_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the concise summary about dataset after some cleaning process\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Missing value imputation using KNNImputer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Features for imputation\n",
    "features = df[['Bathrooms', 'Bedrooms', 'Price','Size_sqft']]\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Fit and transform the features\n",
    "imputed_features = imputer.fit_transform(features)\n",
    "\n",
    "# Convert the imputed features to a DataFrame\n",
    "imputed_features_df = pd.DataFrame(imputed_features, columns=features.columns)\n",
    "\n",
    "# Replace missing values in the original 'Size_sqft' column with the imputed values\n",
    "df['Size_sqft'] = df['Size_sqft'].fillna(imputed_features_df['Size_sqft'])\n",
    "\n",
    "#Check the dataframe null values after imputation\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Even after applying KNNImputer there are 432 missing values. This may due to insufficient Neighbours or sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the remaining nullvalues\n",
    "df.dropna( subset = ['Size_sqft'] , inplace = True )\n",
    "\n",
    "#checking the null values again\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset after removing missing values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Analysing features Bedrooms, Bathrooms and Parking Based on  Unit_type\n",
    "df.groupby('Unit_type')[['Bedrooms', 'Bathrooms', 'Parking']].apply(lambda x: x.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency tables of Bedrooms Vs Bathrooms\n",
    "pd.crosstab(df['Bedrooms'],df['Bathrooms'],margins=True,margins_name='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Frequency table of Bedrooms Vs Unit_Type\n",
    "pd.crosstab(df['Bedrooms'],df['Unit_type'],margins=True,margins_name='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency table of Pet_friendly Vs Unit_Type\n",
    "pd.crosstab(df['Pet_friendly'],df['Unit_type'],margins=True,margins_name='Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Plotting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by Unit_type and count the occurrences of number of Bedrooms\n",
    "grouped = df.groupby(['Unit_type', 'Bedrooms']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.bar(grouped, x='Unit_type', y='count', color='Bedrooms',\n",
    "             title='Count of Bedrooms by Unit Type',\n",
    "             labels={'count': 'Count', 'Unit_type': 'Unit Type'})\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Group by Unit_type and count the number of bathrooms\n",
    "grouped = df.groupby(['Unit_type', 'Bathrooms']).size().reset_index(name='count')\n",
    "fig = px.bar(grouped, x='Unit_type', y='count', color='Bathrooms',\n",
    "             title='Count of Bathrooms by Unit Type',\n",
    "             labels={'count': 'Count', 'Unit_type': 'Unit Type'})\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Group by Unit_type and count number of parking\n",
    "df['Parking'] = df['Parking'].astype('category')\n",
    "grouped = df.groupby(['Unit_type', 'Parking']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.bar(grouped, x='Unit_type', y='count', color='Parking',\n",
    "             title='Count of Parking by Unit Type',\n",
    "             labels={'count': 'Count', 'Unit_type': 'Unit Type'})\n",
    "fig.show()\n",
    "\n",
    "#convert Parking back to integer type\n",
    "df['Parking'] = df['Parking'].astype('int')\n",
    "\n",
    "# Group by Unit_type and count by Pet_friendly\n",
    "grouped = df.groupby(['Unit_type', 'Pet_friendly']).size().reset_index(name='count')\n",
    "\n",
    "# Plotting\n",
    "fig = px.bar(grouped, x='Unit_type', y='count', color='Pet_friendly',\n",
    "             title='Count of Pet Friendly by Unit Type',\n",
    "             labels={'count': 'Count', 'Unit_type': 'Unit Type', 'Pet_friendly': 'Pet Friendly'})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Furnished_type By Unit_types\n",
    "\n",
    "grouped = df.groupby(['Unit_type', 'Furnished_info']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.pie(grouped, values='count', names='Furnished_info', title='Count of Furnished info by Unit Type',\n",
    "             labels={'count': 'Count', 'Furnished_info': 'Furnished_info'}, color='Furnished_info', \n",
    "             facet_col='Unit_type', facet_col_wrap=2)\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting histogram of each numerical variable\n",
    "\n",
    "columns=['Price','Size_sqft','Bedrooms','Bathrooms','Parking']\n",
    "fig,axs=plt.subplots(2,3, figsize=(15,10))\n",
    "axs=axs.flatten()\n",
    "\n",
    "#plt.figure(facecolor='grid',figsize=(20,15))\n",
    "for i, col in enumerate(columns):\n",
    "    ax=axs[i]\n",
    "    df[col].hist(bins=10,ax=ax)\n",
    "\n",
    "    ax.set_title(f'Histogram for {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of Price and Size_sqft\n",
    "sns.scatterplot(x='Size_sqft', y='Price', data=df,hue='Size_sqft',size='Price')\n",
    "plt.show()\n",
    "\n",
    "## It is clear that both Price as well as Size_sqft has outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation matrix\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df.corr(),annot=True, cmap='YlGnBu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot each numerical variable against another\n",
    "sns.pairplot(data=df, hue='Unit_type')\n",
    "plt.suptitle(\"Pairplot with Hue for Unit Type\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 2), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checkig for outliers\n",
    "plt.figure(num= None, figsize=(10,10), dpi=1000, facecolor=\"White\")\n",
    "df.plot(kind =\"box\")\n",
    "plt.title('Boxplot for Outlier Detection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are outlier in Size_sqft and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Boxplot for each type of unit in the X axis and the price in the Y axis\n",
    "plt.figure( figsize = ( 10 , 6 ) ) \n",
    "sns.boxplot( x = \"Unit_type\" , y = \"Price\" , data = df )\n",
    "plt.title('Boxplot for Outlier Detection of Price in Unit_types')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10 , 6 ) ) \n",
    "sns.boxplot( x = \"Bedrooms\" , y = \"Price\" , data = df )\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pair plot\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pandas Profiling and Making Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "\n",
    "# profile = ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_widgets()\n",
    "# profile.to_file('output.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encoding categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#label encoding with Yes/No values (Furnished_info)\n",
    "label_mapping={'Yes':1,'No':0}\n",
    "df['Encoded_Furnished_info']=df['Furnished_info'].map(label_mapping)\n",
    "\n",
    "#encoding for ordinal category variables\n",
    "price_condition_ord_map={'No Utilities Included':1,'Some Utilities Included':2, \n",
    "                         'All Utilities Included':3}\n",
    "pet_friendly_ord_map={'No':1,'Limited':2 ,'Yes':3}\n",
    "\n",
    "df['Encoded_price_condition']=df['Price_condition'].map(price_condition_ord_map)\n",
    "df['Encoded_pet_friendly']=df['Pet_friendly'].map(pet_friendly_ord_map)\n",
    "\n",
    "#One-Hot Encoding to create binary columns for nominal category variable unit_type\n",
    "encoded=pd.get_dummies(df['Unit_type'],prefix='Unit_type')\n",
    "df=pd.concat([df,encoded],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe after encoding\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation matrix\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(df.corr(),annot=True, cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is clear from the correlation matrix that, any of the features are highly correlated to each other. Hence no need to remove any feature for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identify and correct Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Outlier detection using boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing boxplot for each column\n",
    "columns = ['Price' , 'Size_sqft']\n",
    "plt.figure(figsize = (20, 10))\n",
    "for i , col in enumerate( columns ):\n",
    "    # Plotting various columns to analyse and see outliers\n",
    "    plt.subplot( 1 , 2 , i + 1 )\n",
    "    df.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s\"%col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Outlier detection using Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram of each column\n",
    "columns = [ 'Price', 'Size_sqft' ]\n",
    "plt.figure(figsize = (20, 5))\n",
    "for i , col in enumerate( columns ):\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 1 , 2 , i + 1 )\n",
    "    df[col].hist(bins = 10)\n",
    "    plt.title(\"Histogram for %s\"%col)\n",
    "    plt.xlabel(col)\n",
    "plt.show()\n",
    "\n",
    "## it is clear that price and size_sqft having outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Outlier detection using Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scatterplots \n",
    "fig,ax=plt.subplots(figsize=(5,4))\n",
    "ax.scatter(df['Size_sqft'],df['Price'])\n",
    "ax.set_xlabel('Size Sqft')\n",
    "ax.set_ylabel('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Outlier detection using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier detection-IQR\n",
    "use_columns = [ 'Price', 'Size_sqft' ]\n",
    "q1=df[use_columns].quantile(0.25)\n",
    "q3=df[use_columns].quantile(0.75)\n",
    "IQR=q3-q1\n",
    "Lower_Whisker=q1-(1.5*IQR)\n",
    "Upper_Whisker=q3+(1.5*IQR)\n",
    "print(\"First quartile for each use column :\")\n",
    "print( q1 )\n",
    "print(\"\\nThird quartile for each use column :\")\n",
    "print( q3 )\n",
    "print(\"\\nInterquartile Range for each use column :\")\n",
    "print( IQR )\n",
    "print(\"\\nLower Whisker for each use column :\")\n",
    "print( Lower_Whisker )\n",
    "print(\"\\nUpper Whisker for each use column :\")\n",
    "print( Upper_Whisker )\n",
    "outliers_finder = ((df[use_columns] < Lower_Whisker) | (df[use_columns] > Upper_Whisker) ).any(axis=1)\n",
    "outliers_result = df[outliers_finder]\n",
    "#the outliers present in the dataset\n",
    "print(\"\\n\\nnumber of outliers: \",outliers_result.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset with removed outliers \n",
    "removed_outlier= ( (df[use_columns] >= Lower_Whisker) & (df[use_columns] <= Upper_Whisker) ).all(axis=1)\n",
    "df_removed_outlier=df[removed_outlier]\n",
    "print('Count of df after removing  outlier: ',df_removed_outlier.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since the number of Outliers are high, It is not the good practice to simply eliminate all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drawing histogram for each column for data with and without outliers\n",
    "columns = [ 'Price' ,'Size_sqft']\n",
    "plt.figure(figsize = (20, 15))\n",
    "for i , col in enumerate( columns ):\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 1 )\n",
    "    df[col].hist(bins = 20)\n",
    "    plt.title(\"Histogram for %s with outliers\"%col)\n",
    "    plt.xlabel(col)\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 2 )\n",
    "    df_removed_outlier[col].hist(bins = 20)\n",
    "    plt.title(\"Histogram for %s without outliers\"%col)\n",
    "    plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare using Boxplot\n",
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "plt.figure(figsize = (10, 8))\n",
    "for i , col in enumerate( use_columns ):\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 1 )\n",
    "    df.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s without floor and cap\"%col)\n",
    "    #plt.xlabel(col)\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 2 )\n",
    "    df_removed_outlier.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s with floor and cap\"%col)\n",
    "    #plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scatter plot after removing outlier\n",
    "#using scatterplots \n",
    "fig,ax=plt.subplots(1,2, figsize=(15,8))\n",
    "ax[0].scatter(df['Size_sqft'],df['Price'])\n",
    "ax[0].set_xlabel('Size Sqft')\n",
    "ax[0].set_ylabel('Price')\n",
    "ax[0].set_title('Scatterplot without outlier removal')\n",
    "\n",
    "\n",
    "ax[1].scatter(df_removed_outlier['Size_sqft'],df_removed_outlier['Price'])\n",
    "ax[1].set_xlabel('Size Sqft')\n",
    "ax[1].set_ylabel('Price')\n",
    "ax[1].set_title('Scatterplot with outlier removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Treatments: Quantile based flooring& capping, Trimming and Log Transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Quantile based flooring and capping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "floor = df[use_columns].quantile(0.01)\n",
    "cap   = df[use_columns].quantile(0.99)\n",
    "\n",
    "print('floor:\\n',floor)\n",
    "print('\\ncap:\\n',cap)\n",
    "\n",
    "floor_cap = ((df[use_columns] > floor) & (df[use_columns] < cap) ).all(axis=1)\n",
    "floor_cap_df = df[floor_cap]\n",
    "print('\\nShape of floor_cap_df:',floor_cap_df.shape)\n",
    "print('\\nShape of df:',df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of dataset after floor and cap: \",floor_cap_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drawing histogram for price and size_sqft\n",
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "plt.figure(figsize = (10, 8))\n",
    "for i , col in enumerate(use_columns):\n",
    "    \n",
    "    # Plotting histogram to  demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 1 )\n",
    "    df[col].hist(bins = 20, color='purple')\n",
    "    plt.title(\"Histogram for %s without floor and cap\"%col)\n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    # Plotting histogram  to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 2 )\n",
    "    floor_cap_df[col].hist(bins = 20,color='orange')\n",
    "    plt.title(\"Histogram for %s with floor and cap\"%col)\n",
    "    plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot after floor cap\n",
    "#using scatterplots \n",
    "fig,ax=plt.subplots(1,2, figsize=(15,8))\n",
    "ax[0].scatter(df['Size_sqft'],df['Price'],c='blue', cmap='rainbow')\n",
    "ax[0].set_xlabel('Size Sqft')\n",
    "ax[0].set_ylabel('Price')\n",
    "ax[0].set_title('Without floor cap')\n",
    "\n",
    "\n",
    "ax[1].scatter(floor_cap_df['Size_sqft'],floor_cap_df['Price'],c='pink', cmap='rainbow')\n",
    "ax[1].set_xlabel('Size Sqft')\n",
    "ax[1].set_ylabel('Price')\n",
    "ax[1].set_title('With floor cap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drawing BOX - PLOT for each column\n",
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "plt.figure(figsize = (10, 8))\n",
    "for i , col in enumerate( use_columns ):\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 1 )\n",
    "    df.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s without floor and cap\"%col)\n",
    "    #plt.xlabel(col)\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 2 )\n",
    "    floor_cap_df.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s with floor and cap\"%col)\n",
    "    #plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "df2 = df.copy()\n",
    "idx_trimmed = ( (df2[use_columns] >=Lower_Whisker) & (df2[use_columns] <=Upper_Whisker) ).all(axis=1)\n",
    "trimmed_df = df2[idx_trimmed]\n",
    "print('original dataframe: ',df2.shape)\n",
    "print('Trimmed dataframe: ',trimmed_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drawing BOX - PLOT for each column\n",
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "plt.figure(figsize = (10, 8))\n",
    "for i , col in enumerate( use_columns ):\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 1 )\n",
    "    df.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s without trimming \"%col)\n",
    "    #plt.xlabel(col)\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , 2*i + 2 )\n",
    "    trimmed_df.boxplot(column=col)\n",
    "    plt.title(\"Boxplot for %s after trimming\"%col)\n",
    "    #plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot before and after trimming\n",
    "#using scatterplots \n",
    "\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,8))\n",
    "ax[0].scatter(df['Size_sqft'],df['Price'])\n",
    "ax[0].set_xlabel('Size Sqft')\n",
    "ax[0].set_ylabel('Price')\n",
    "ax[0].set_title('Before trimming')\n",
    "\n",
    "\n",
    "ax[1].scatter(trimmed_df['Size_sqft'],trimmed_df['Price'],c='magenta')\n",
    "ax[1].set_xlabel('Size Sqft')\n",
    "ax[1].set_ylabel('Price')\n",
    "ax[1].set_title('After Trimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drawing histogram for price and size_sqft\n",
    "use_columns = [ 'Price','Size_sqft' ]\n",
    "plt.figure(figsize = (10, 8))\n",
    "for i , col in enumerate(use_columns):\n",
    "    \n",
    "    # Plotting histogram to  demonstrate distributions before trimming\n",
    "    plt.subplot( 2 , 2 , 2*i + 1 )\n",
    "    df[col].hist(bins = 20)\n",
    "    plt.title(\"Histogram for %s before trimming\"%col)\n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    # Plotting histogram  to demonstrate distributions after trimming\n",
    "    plt.subplot( 2 , 2 , 2*i + 2 )\n",
    "    trimmed_df[col].hist(bins = 20)\n",
    "    plt.title(\"Histogram for %s after trimming \"%col)\n",
    "    plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Price\n",
    "print( \" Price Skew : \" , log_df[\"Price\"].skew().round(2) )\n",
    "\n",
    "# For Size Sqft\n",
    "print( \"Size Sqft Skew  : \" , log_df[\"Size_sqft\"].skew().round(2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Price\n",
    "log_df[\"log_price\"] = log_df[\"Price\"].map( lambda i : np.log(i) if i > 0 else 0 )\n",
    "print( \"Log Price Skew : \" , log_df[\"log_price\"].skew().round(2) )\n",
    "\n",
    "### For Size_sqft\n",
    "log_df[\"log_size_sqft\"] = log_df[\"Size_sqft\"].map( lambda i : np.log(i) if i > 0 else 0 )\n",
    "print( \"Log Size Sqft Skew : \" , log_df[\"log_size_sqft\"].skew().round(2) )\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataframe: ',df.shape)\n",
    "print('Log transformed dataframe: ',log_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drawing histogram for price and size_sqft\n",
    "use_columns = ['Price','Size_sqft','log_price','log_size_sqft']\n",
    "plt.figure(figsize = (15, 12))\n",
    "for i , col in enumerate( use_columns ):\n",
    "    # Plotting various columns to demonstrate distributions\n",
    "    plt.subplot( 2 , 2 , i + 1 )\n",
    "    log_df[col].hist(bins = 20)\n",
    "    plt.title(\"Histogram for %s\"%col)\n",
    "    plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the skewness for Price and Size_sqft when using floor _cap method \n",
    "print('Skewness of Price: ',floor_cap_df['Price'].skew().round(2))\n",
    "print('Skewness of Size_sqrt: ',floor_cap_df['Size_sqft'].skew().round(2))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The skewness of Sqft is high. Inorder to remove the skewness, log tranform is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "floor_cap_df['log_Size_sqft']=floor_cap_df['Size_sqft'].map(lambda i: np.log(i) if i>0 else 0)\n",
    "print(floor_cap_df['log_Size_sqft'].skew().round(2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Even after doing log transformation Floor_caped data have skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the skewness for Price and Size_sqft when using floor _cap method \n",
    "print('Skewness of Price: ',trimmed_df['Price'].skew().round(2))\n",
    "print('Skewness of Size_sqrt: ',trimmed_df['Size_sqft'].skew().round(2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comparison of Outlier techniques:\n",
    "1. Cap and floor method, eventhough the data lose is less, but the column Size_sqft is highly skewd(1.4). The skewness couldn't removed even after applying log transform. Hence, this method is not using for further analysis.\n",
    "2. Trimming method, the data lose is little bit higher, but the skewness is low. \n",
    "3.Log transformation , the skewness for both data is very high(-4.41 and -4.54). \n",
    "Finally,  Trimming method is chosen for further processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df = trimmed_df.drop(columns=['Title','Price_condition','Unit_type','Pet_friendly','Furnished_info','Description'])\n",
    "selected_columns =['Unit_type_Apartment','Unit_type_Basement','Unit_type_Condo','Unit_type_Duplex/Triplex','Unit_type_House','Unit_type_Townhouse']\n",
    "kmeans_df[selected_columns] = kmeans_df[selected_columns].apply(lambda x: x.astype(int))\n",
    "kmeans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "kmeans_df['Size_sqft']=scaler.fit_transform(kmeans_df[['Size_sqft']])\n",
    "kmeans_df['Price']=scaler.fit_transform(kmeans_df[['Price']])\n",
    "kmeans_df.head()\n",
    "dbscan_df=kmeans_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias=[]\n",
    "for i in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(kmeans_df[['Bedrooms', 'Bathrooms', 'Parking', 'Size_sqft', 'Price',\n",
    "       'Encoded_Furnished_info', 'Encoded_price_condition',\n",
    "       'Encoded_pet_friendly', 'Unit_type_Apartment', 'Unit_type_Basement',\n",
    "       'Unit_type_Condo', 'Unit_type_Duplex/Triplex', 'Unit_type_House',\n",
    "       'Unit_type_Townhouse']])\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(x=range(1, 15), y=inertias)\n",
    "\n",
    "# Adding axis labels and title\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Centroids\",\n",
    "    yaxis_title=\"Error\",\n",
    "    title=\"No. of Centroids vs Error\",\n",
    ")\n",
    "\n",
    "# Adding markers\n",
    "fig.update_traces(mode='markers+lines')\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=17, n_init=\"auto\")\n",
    "kmeans.fit_transform(kmeans_df)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df['Cluster']=kmeans.predict(kmeans_df)\n",
    "kmeans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kmeans_df['Size_sqft'],kmeans_df['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(kmeans_df, x='Size_sqft', y='Price',title='Price Vs Size_sqft scatter plot', color='Cluster')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(kmeans_df, x='Size_sqft', y='Price', z='Bedrooms', color='Cluster',\n",
    "                    title=\"3D Scatter Plot - Price vs Size_Sqft vs Bedroom\",\n",
    "                    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(kmeans_df, x='Price', y='Parking', z='Bedrooms', color='Cluster',\n",
    "                    title=\"3D Scatter Plot - Price vs Parking vs Bedroom\",\n",
    "                    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan_df['Cluster'] = DBSCAN(eps=0.08, min_samples=5).fit_predict(dbscan_df[['Size_sqft','Price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(dbscan_df, x='Size_sqft', y='Price', color='Cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(dbscan_df, x='Size_sqft', y='Price', z='Bedrooms', color='Cluster',\n",
    "                    title=\"3D Scatter Plot - Price vs Size_Sqft vs Bedroom\",\n",
    "                    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libs for clustering and plotting\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "agglomerative_df = kmeans_df.drop(columns=['Cluster']).copy()\n",
    "\n",
    "points=agglomerative_df.values.tolist()\n",
    "# Plotting dendrogram\n",
    "dendrogram =sch.dendrogram(sch.linkage(points,method='ward'))\n",
    "plt.ylabel('Distance')\n",
    "plt.xlabel('Data Points',labelpad=20)\n",
    "plt.title('Cluster Dendrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg= AgglomerativeClustering(n_clusters=5,linkage='ward')\n",
    "agglomerative_df['Cluster'] =agg.fit_predict(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(agglomerative_df, x='Size_sqft', y='Price', color='Cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(agglomerative_df, x='Size_sqft', y='Price', z='Bedrooms', color='Cluster',\n",
    "                    title=\"3D Scatter Plot - Price vs Size_Sqft vs Bedroom\",\n",
    "                    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. NLP Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load df into new dataframe nlp_df\n",
    "nlp_df=trimmed_df[['Description']][:300].copy()\n",
    "nlp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "nlp_df['Description'] = nlp_df['Description'].apply(lambda x: x.replace('\\n',\" \"))\n",
    "nlp_df['raw_Description'] =nlp_df['Description']\n",
    "nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopwords=list(stopwords.words('english'))\n",
    "custom_stopwords=['description','parking','room','suite','apartment','bedroom','living','building','Available','unit','laundry',\n",
    "                  'available','rent','park','located','suite','included','one','many','lt','apartment'\n",
    "                  ]\n",
    "\n",
    "stopwords=stopwords+custom_stopwords\n",
    "def stopword_removal(text):\n",
    "    ## punctuation removal\n",
    "    pun_removed_text=text.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    word_token=[word.lower() for word in word_tokenize(pun_removed_text) if ((word.lower() not in stopwords)and len(word)>2)]\n",
    "    \n",
    "    return word_token\n",
    "    \n",
    "\n",
    "nlp_df['cleaned_description'] = nlp_df['Description'].apply(lambda x:' '.join(stopword_removal(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_df['cleaned_description'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji #For emoji to word conversion\n",
    "from spellchecker import SpellChecker #For checking spelling and apply fixes\n",
    "\n",
    "\n",
    "#Downloading emoji dataset along with emojies denifition/name\n",
    "demoji.download_codes()\n",
    "\n",
    "# Initialize SpellChecker for correcting word spellings\n",
    "spellChecker = SpellChecker()\n",
    "\n",
    "\n",
    "nlp_df['cleaned_description'] = nlp_df['cleaned_description'].apply(lambda description: demoji.replace_with_desc(description))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digitRemover(text):\n",
    "    words = word_tokenize(text)\n",
    "    temp=[]\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'\\d', '', word)\n",
    "        temp.append(new_word)\n",
    "\n",
    "    return temp\n",
    "\n",
    "nlp_df['cleaned_description'] = nlp_df['cleaned_description'].apply(lambda description: digitRemover(description))\n",
    "nlp_df['cleaned_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(values):\n",
    "    val = spacy_nlp(\" \".join(values))\n",
    "    temp=[]\n",
    "    for token in val:\n",
    "        lemma = token.lemma_\n",
    "        temp.append(lemma)\n",
    "    return temp\n",
    "\n",
    "\n",
    "nlp_df['cleaned_description'] = nlp_df['cleaned_description'].apply(lambda description: lemm(description) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nlp_df['cleaned_description'].apply(lambda x: ' '.join(x)).tolist()\n",
    "all_words=\" \".join(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=pd.DataFrame({'all_words':word_tokenize(all_words)}) \n",
    "all_words.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts_df = pd.DataFrame(all_words['all_words'].value_counts())\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(value_counts_df.iloc[:15][::-1], x='count', title='Bar Graph')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "value_counts_df = value_counts_df.reset_index()\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def ngramsPlotter(corpus):\n",
    "    # Tokenize it into words\n",
    "    words = corpus.lower().split()\n",
    "\n",
    "    # Create lists of unigrams, bigrams, and trigrams\n",
    "    unigrams_list = list(ngrams(words, 1))\n",
    "    bigrams_list = list(ngrams(words, 2))\n",
    "    trigrams_list = list(ngrams(words, 3))\n",
    "\n",
    "    # Count the frequency of each unigram, bigram, and trigram\n",
    "    unigram_list_counts = collections.Counter(unigrams_list)\n",
    "    bigram_list_counts = collections.Counter(bigrams_list)\n",
    "    trigram_list_counts = collections.Counter(trigrams_list)\n",
    "\n",
    "    # Plot the horizontal bar charts for top 10 ngrams\n",
    "    fig, axs = plt.subplots(3, figsize=(10, 15))\n",
    "\n",
    "    top_10_unigrams = dict(unigram_list_counts.most_common(20))\n",
    "    axs[0].barh([str(k) for k in top_10_unigrams.keys()], top_10_unigrams.values())\n",
    "    axs[0].set_title(\"Top 10 Unigrams\")\n",
    "    axs[0].set_xlabel(\"Words\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    top_10_bigrams = dict(bigram_list_counts.most_common(20))\n",
    "    axs[1].barh([str(k) for k in top_10_bigrams.keys()], top_10_bigrams.values())\n",
    "    axs[1].set_title(\"Top 10 Bigrams\")\n",
    "    axs[1].set_xlabel(\"Bigrams\")\n",
    "    axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    top_10_trigrams = dict(trigram_list_counts.most_common(20))\n",
    "    axs[2].barh([str(k) for k in top_10_trigrams.keys()], top_10_trigrams.values())\n",
    "    axs[2].set_title(\"Top 10 Trigrams\")\n",
    "    axs[2].set_xlabel(\"Trigrams\")\n",
    "    axs[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "item = [items for subItems in nlp_df['cleaned_description'].tolist() for items in subItems]\n",
    "ngramsPlotter(\" \".join(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
